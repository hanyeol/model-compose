controller:
  type: http-server
  port: 8080
  base_path: /api
  webui:
    driver: gradio
    port: 8081

workflow:
  title: Chat with GLM-4.7 on vLLM
  description: Generate text responses using GLM-4.7 model on vLLM server
  input: ${input}
  output: ${output as text;sse-text}

component:
  type: http-server
  manage:
    install:
      - bash
      - -c
      - |
        eval "$(pyenv init -)" &&
        (pyenv activate vllm 2>/dev/null || pyenv virtualenv $(python --version | cut -d' ' -f2) vllm) &&
        pyenv activate vllm &&
        pip install --no-cache-dir --upgrade --force-reinstall "vllm @ https://wheels.vllm.ai/ade81f17feeebef775e8cddf9a8f23848ec694a3/vllm-0.16.0rc2.dev502%2Bgade81f17f.cu130-cp38-abi3-manylinux_2_35_aarch64.whl"
    start:
      - bash
      - -c
      - |
        eval "$(pyenv init -)" &&
        pyenv activate vllm &&
        vllm serve zai-org/GLM-4.7-FP8 \
          --port 8000 \
          --served-model-name glm-4.7 \
          --max-model-len 16384 \
          --gpu-memory-utilization 0.95 \
          --speculative-config.method mtp \
          --speculative-config.num_speculative_tokens 1
  port: 8000
  method: POST
  path: /v1/chat/completions
  headers:
    Content-Type: application/json
  body:
    model: glm-4.7
    messages:
      - role: user
        content: ${input.prompt as text}
    max_tokens: 4096
    temperature: ${input.temperature as number | 0.7}
    stream: true
  stream_format: json
  output: ${response[].choices[0].delta.content}
